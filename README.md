Home Sales Challenge

Contributor: Gabriela Delgado

Overview

This project was meant to use a dataset to create tables based on information of interest. Then, cache and parquet the dataset to be able to compare efficiencies based on the time it took to run the code in the different forms of datasets.

This project includes a Jupyter Notebook document with the code and a ReadMe file for context about this repository.

Git Bash

I used GitBash to download the repository to my desktop and push and commit the changes made to the code into the repository

Jupyter Notebook & Google Colaboratory

These programs were used to write and run the code for spark.sql to create the tables, temporary views, cache and parquet the data. The model should work on the platform of preference but it is more convenient to use Google Colaboratory which will create the Jupyter Notebook there.

Acknowledgments:

Tutor Kourt Bailey for answering my questions about how to use Google Colaboratory and debug my code.
